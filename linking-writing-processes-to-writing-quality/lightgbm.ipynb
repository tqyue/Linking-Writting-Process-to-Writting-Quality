{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "导入库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import gc\n",
    "import os\n",
    "import itertools\n",
    "import pickle\n",
    "import re\n",
    "import time\n",
    "from random import choice, choices\n",
    "from functools import reduce\n",
    "from tqdm import tqdm\n",
    "from itertools import cycle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from functools import reduce\n",
    "from itertools import cycle\n",
    "from scipy import stats\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn import metrics, model_selection, preprocessing, linear_model, ensemble, decomposition, tree\n",
    "import lightgbm as lgb\n",
    "import torch\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取数据加入至内存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = './'\n",
    "train_logs = pd.read_csv(f'{INPUT_DIR}/train_logs.csv')\n",
    "train_scores = pd.read_csv(f'{INPUT_DIR}/train_scores.csv')\n",
    "test_logs = pd.read_csv(f'{INPUT_DIR}/test_logs.csv')\n",
    "ss_df = pd.read_csv(f'{INPUT_DIR}/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>essay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>001519c8</td>\n",
       "      <td>qqqqqqqqq qq qqqqq qq qqqq qqqq.  qqqqqq qqq q...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0022f953</td>\n",
       "      <td>qqqq qq qqqqqqqqqqq ? qq qq qqq qqq qqq, qqqqq...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0042269b</td>\n",
       "      <td>qqqqqqqqqqq qq qqqqq qqqqqqqqq qq qqqqqqqqqqq ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0059420b</td>\n",
       "      <td>qq qqqqqqq qqqqqq qqqqqqqqqqqqq qqqq q qqqq qq...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0075873a</td>\n",
       "      <td>qqqqqqqqqqq qq qqq qqqqq qq qqqqqqqqqq, qqq qq...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                              essay\n",
       "0  001519c8  qqqqqqqqq qq qqqqq qq qqqq qqqq.  qqqqqq qqq q...\n",
       "1  0022f953  qqqq qq qqqqqqqqqqq ? qq qq qqq qqq qqq, qqqqq...\n",
       "2  0042269b  qqqqqqqqqqq qq qqqqq qqqqqqqqq qq qqqqqqqqqqq ...\n",
       "3  0059420b  qq qqqqqqq qqqqqq qqqqqqqqqqqqq qqqq q qqqq qq...\n",
       "4  0075873a  qqqqqqqqqqq qq qqq qqqqq qq qqqqqqqqqq, qqq qq..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_essays = pd.read_csv(f\"{INPUT_DIR}/trained essay_info.csv\")\n",
    "train_essays.index = train_essays[\"Unnamed: 0\"]\n",
    "train_essays.index.name = None\n",
    "train_essays.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
    "train_essays.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "初始特征的类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = ['down_time', 'up_time', 'action_time', 'cursor_position', 'word_count']\n",
    "activities = ['Input', 'Remove/Cut', 'Nonproduction', 'Replace', 'Paste']\n",
    "events = ['q', 'Space', 'Backspace', 'Shift', 'ArrowRight', 'Leftclick', 'ArrowLeft', '.', ',', 'ArrowDown', 'ArrowUp', 'Enter', 'CapsLock', \"'\", 'Delete', 'Unidentified']\n",
    "text_changes = ['q', ' ', '.', ',', '\\n', \"'\", '\"', '-', '?', ';', '=', '/', '\\\\', ':']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "def q1(x):\n",
    "    return x.quantile(0.25)\n",
    "def q3(x):\n",
    "    return x.quantile(0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对文本进行特征生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EssayConstructor:\n",
    "    \n",
    "    def processingInputs(self,currTextInput):\n",
    "        # Where the essay content will be stored\n",
    "        essayText = \"\"\n",
    "        # Produces the essay\n",
    "        for Input in currTextInput.values:\n",
    "            # Input[0] = activity\n",
    "            # Input[1] = cursor_position\n",
    "            # Input[2] = text_change\n",
    "            # Input[3] = id\n",
    "            # If activity = Replace\n",
    "            if Input[0] == 'Replace':\n",
    "                # splits text_change at ' => '\n",
    "                replaceTxt = Input[2].split(' => ')\n",
    "                # DONT TOUCH\n",
    "                essayText = essayText[:Input[1] - len(replaceTxt[1])] + replaceTxt[1] + essayText[Input[1] - len(replaceTxt[1]) + len(replaceTxt[0]):]\n",
    "                continue\n",
    "\n",
    "            # If activity = Paste    \n",
    "            if Input[0] == 'Paste':\n",
    "                # DONT TOUCH\n",
    "                essayText = essayText[:Input[1] - len(Input[2])] + Input[2] + essayText[Input[1] - len(Input[2]):]\n",
    "                continue\n",
    "\n",
    "            # If activity = Remove/Cut\n",
    "            if Input[0] == 'Remove/Cut':\n",
    "                # DONT TOUCH\n",
    "                essayText = essayText[:Input[1]] + essayText[Input[1] + len(Input[2]):]\n",
    "                continue\n",
    "\n",
    "            # If activity = Move...\n",
    "            if \"M\" in Input[0]:\n",
    "                # Gets rid of the \"Move from to\" text\n",
    "                croppedTxt = Input[0][10:]              \n",
    "                # Splits cropped text by ' To '\n",
    "                splitTxt = croppedTxt.split(' To ')              \n",
    "                # Splits split text again by ', ' for each item\n",
    "                valueArr = [item.split(', ') for item in splitTxt]              \n",
    "                # Move from [2, 4] To [5, 7] = (2, 4, 5, 7)\n",
    "                moveData = (int(valueArr[0][0][1:]), int(valueArr[0][1][:-1]), int(valueArr[1][0][1:]), int(valueArr[1][1][:-1]))\n",
    "                # Skip if someone manages to activiate this by moving to same place\n",
    "                if moveData[0] != moveData[2]:\n",
    "                    # Check if they move text forward in essay (they are different)\n",
    "                    if moveData[0] < moveData[2]:\n",
    "                        # DONT TOUCH\n",
    "                        essayText = essayText[:moveData[0]] + essayText[moveData[1]:moveData[3]] + essayText[moveData[0]:moveData[1]] + essayText[moveData[3]:]\n",
    "                    else:\n",
    "                        # DONT TOUCH\n",
    "                        essayText = essayText[:moveData[2]] + essayText[moveData[0]:moveData[1]] + essayText[moveData[2]:moveData[0]] + essayText[moveData[1]:]\n",
    "                continue                \n",
    "                \n",
    "            # If activity = input\n",
    "            # DONT TOUCH\n",
    "            essayText = essayText[:Input[1] - len(Input[2])] + Input[2] + essayText[Input[1] - len(Input[2]):]\n",
    "        return essayText\n",
    "            \n",
    "            \n",
    "    def getEssays(self,df):\n",
    "        # Copy required columns\n",
    "        textInputDf = copy.deepcopy(df[['id', 'activity', 'cursor_position', 'text_change']])\n",
    "        # Get rid of text inputs that make no change\n",
    "        textInputDf = textInputDf[textInputDf.activity != 'Nonproduction']     \n",
    "        # construct essay, fast \n",
    "        tqdm.pandas()\n",
    "        essay=textInputDf.groupby('id')[['activity','cursor_position', 'text_change']].progress_apply(lambda x: self.processingInputs(x))      \n",
    "        # to dataframe\n",
    "        essayFrame=essay.to_frame().reset_index()\n",
    "        essayFrame.columns=['id','essay']\n",
    "        # Returns the essay series\n",
    "        return essayFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "AGGREGATIONS = ['count', 'mean', 'std', 'min', 'max', 'first', 'last', 'sem', q1, 'median', q3, 'skew', pd.DataFrame.kurt, 'sum']\n",
    "\n",
    "def split_essays_into_words(df):\n",
    "    essay_df = df\n",
    "    essay_df['word'] = essay_df['essay'].apply(lambda x: re.split(' |\\\\n|\\\\.|\\\\?|\\\\!',x))\n",
    "    essay_df = essay_df.explode('word')\n",
    "    essay_df['word_len'] = essay_df['word'].apply(lambda x: len(x))\n",
    "    essay_df = essay_df[essay_df['word_len'] != 0]\n",
    "    return essay_df\n",
    "\n",
    "def compute_word_aggregations(word_df):\n",
    "    word_agg_df = word_df[['id','word_len']].groupby(['id']).agg(AGGREGATIONS)\n",
    "    word_agg_df.columns = ['_'.join(x) for x in word_agg_df.columns]\n",
    "    word_agg_df['id'] = word_agg_df.index\n",
    "    for word_l in [5, 6, 7, 8, 9, 10, 11, 12]:\n",
    "        word_agg_df[f'word_len_ge_{word_l}_count'] = word_df[word_df['word_len'] >= word_l].groupby(['id']).count().iloc[:, 0]\n",
    "        word_agg_df[f'word_len_ge_{word_l}_count'] = word_agg_df[f'word_len_ge_{word_l}_count'].fillna(0)\n",
    "    word_agg_df = word_agg_df.reset_index(drop=True)\n",
    "    return word_agg_df\n",
    "\n",
    "def compute_product_word_cnt(word_df):\n",
    "    # 按文章 ID 分组，并计算每篇文章的单词数量\n",
    "    word_count_per_essay = word_df.groupby('id')['word'].count().reset_index()\n",
    "    word_count_per_essay.columns = ['id', 'product_cnt']\n",
    "    return word_count_per_essay\n",
    "\n",
    "def split_essays_into_sentences(df):\n",
    "    essay_df = df\n",
    "    essay_df['id'] = essay_df.index\n",
    "    essay_df['sent'] = essay_df['essay'].apply(lambda x: re.split('\\\\.|\\\\?|\\\\!',x))\n",
    "    essay_df = essay_df.explode('sent')\n",
    "    essay_df['sent'] = essay_df['sent'].apply(lambda x: x.replace('\\n','').strip())\n",
    "    # Number of characters in sentences\n",
    "    essay_df['sent_len'] = essay_df['sent'].apply(lambda x: len(x))\n",
    "    # Number of words in sentences\n",
    "    essay_df['sent_word_count'] = essay_df['sent'].apply(lambda x: len(x.split(' ')))\n",
    "    essay_df = essay_df[essay_df.sent_len!=0].reset_index(drop=True)\n",
    "    return essay_df\n",
    "\n",
    "def compute_sentence_aggregations(df):\n",
    "    sent_agg_df = pd.concat(\n",
    "        [df[['id','sent_len']].groupby(['id']).agg(AGGREGATIONS), df[['id','sent_word_count']].groupby(['id']).agg(AGGREGATIONS)], axis=1\n",
    "    )\n",
    "    sent_agg_df.columns = ['_'.join(x) for x in sent_agg_df.columns]\n",
    "    sent_agg_df['id'] = sent_agg_df.index\n",
    "    sent_agg_df = sent_agg_df.reset_index(drop=True)\n",
    "    sent_agg_df.drop(columns=[\"sent_word_count_count\"], inplace=True)\n",
    "    sent_agg_df = sent_agg_df.rename(columns={\"sent_len_count\":\"sent_count\"})\n",
    "    return sent_agg_df\n",
    "\n",
    "def split_essays_into_paragraphs(df):\n",
    "    essay_df = df\n",
    "    essay_df['id'] = essay_df.index\n",
    "    essay_df['paragraph'] = essay_df['essay'].apply(lambda x: x.split('\\n'))\n",
    "    essay_df = essay_df.explode('paragraph')\n",
    "    # Number of characters in paragraphs\n",
    "    essay_df['paragraph_len'] = essay_df['paragraph'].apply(lambda x: len(x)) \n",
    "    # Number of words in paragraphs\n",
    "    essay_df['paragraph_word_count'] = essay_df['paragraph'].apply(lambda x: len(x.split(' ')))\n",
    "    essay_df = essay_df[essay_df.paragraph_len!=0].reset_index(drop=True)\n",
    "    return essay_df\n",
    "\n",
    "def compute_paragraph_aggregations(df):\n",
    "    paragraph_agg_df = pd.concat(\n",
    "        [df[['id','paragraph_len']].groupby(['id']).agg(AGGREGATIONS), df[['id','paragraph_word_count']].groupby(['id']).agg(AGGREGATIONS)], axis=1\n",
    "    ) \n",
    "    paragraph_agg_df.columns = ['_'.join(x) for x in paragraph_agg_df.columns]\n",
    "    paragraph_agg_df['id'] = paragraph_agg_df.index\n",
    "    paragraph_agg_df = paragraph_agg_df.reset_index(drop=True)\n",
    "    paragraph_agg_df.drop(columns=[\"paragraph_word_count_count\"], inplace=True)\n",
    "    paragraph_agg_df = paragraph_agg_df.rename(columns={\"paragraph_len_count\":\"paragraph_count\"})\n",
    "    return paragraph_agg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对训练集和测试集进行文本特征产生"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 1502.44it/s]\n"
     ]
    }
   ],
   "source": [
    "essayconstract=EssayConstructor()\n",
    "# Paragraph features for train dataset\n",
    "train_paragraph_df = split_essays_into_paragraphs(train_essays)\n",
    "train_paragraph_agg_df = compute_paragraph_aggregations(train_paragraph_df)\n",
    "\n",
    "# Word features for train dataset\n",
    "#train_word_df = split_essays_into_words(train_essays)\n",
    "#train_word_agg_df = compute_word_aggregations(train_word_df)\n",
    "\n",
    "# Sentence features for train dataset\n",
    "train_sent_df = split_essays_into_sentences(train_essays)\n",
    "train_sent_agg_df = compute_sentence_aggregations(train_sent_df)\n",
    "\n",
    "\n",
    "# Features for test dataset\n",
    "test_essays = essayconstract.getEssays(test_logs)\n",
    "test_sent_agg_df = compute_sentence_aggregations(split_essays_into_sentences(test_essays))\n",
    "#test_word_agg_df=compute_word_aggregations(split_essays_into_words(test_essays))\n",
    "test_paragraph_agg_df = compute_paragraph_aggregations(split_essays_into_paragraphs(test_essays))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "处理统计学特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "class Preprocessor:\n",
    "    \n",
    "    def __init__(self, seed):\n",
    "        self.seed = seed\n",
    "        \n",
    "        self.activities = ['Input', 'Remove/Cut', 'Nonproduction', 'Replace', 'Paste']\n",
    "        self.events = ['q', 'Space', 'Backspace', 'Shift', 'ArrowRight', 'Leftclick', 'ArrowLeft', '.', ',', \n",
    "              'ArrowDown', 'ArrowUp', 'Enter', 'CapsLock', \"'\", 'Delete', 'Unidentified']\n",
    "        self.text_changes = ['q', ' ', 'NoChange', '.', ',', '\\n', \"'\", '\"', '-', '?', ';', '=', '/', '\\\\', ':']\n",
    "        self.punctuations = ['\"', '.', ',', \"'\", '-', ';', ':', '?', '!', '<', '>', '/',\n",
    "                        '@', '#', '$', '%', '^', '&', '*', '(', ')', '_', '+']\n",
    "        self.gaps = [1, 2, 3, 5, 10, 20, 50, 100]\n",
    "        self.percentiles = [5, 10, 25, 50, 75, 90, 95]\n",
    "        self.idf = defaultdict(float)\n",
    "    \n",
    "    #对于每个id对应的activity进行TF-IDF构建特征\n",
    "    def activity_counts(self, df):\n",
    "        tmp_df = df.groupby('id').agg({'activity': list}).reset_index()\n",
    "        ret = list()\n",
    "        for li in tqdm(tmp_df['activity'].values):\n",
    "            items = list(Counter(li).items())\n",
    "            di = dict()\n",
    "            for k in self.activities:\n",
    "                di[k] = 0\n",
    "            for item in items:\n",
    "                k, v = item[0], item[1]\n",
    "                if k in di:\n",
    "                    di[k] = v\n",
    "            ret.append(di)\n",
    "        ret = pd.DataFrame(ret)\n",
    "        cols = [f'activity_{i}_count' for i in range(len(ret.columns))]\n",
    "        ret.columns = cols\n",
    "\n",
    "        cnts = ret.sum(1)\n",
    "\n",
    "        for col in cols:\n",
    "            if col in self.idf.keys():\n",
    "                idf = self.idf[col]\n",
    "            else:\n",
    "                idf = df.shape[0] / (ret[col].sum() + 1)\n",
    "                idf = np.log(idf)\n",
    "                self.idf[col] = idf\n",
    "\n",
    "            ret[col] = 1 + np.log(ret[col] / cnts)\n",
    "            ret[col] *= idf\n",
    "\n",
    "        return ret\n",
    "\n",
    "    #对于每个按键进行TF-IDF构建\n",
    "    def event_counts(self, df, colname):\n",
    "        tmp_df = df.groupby('id').agg({colname: list}).reset_index()\n",
    "        ret = list()\n",
    "        for li in tqdm(tmp_df[colname].values):\n",
    "            items = list(Counter(li).items())\n",
    "            di = dict()\n",
    "            for k in self.events:\n",
    "                di[k] = 0\n",
    "            for item in items:\n",
    "                k, v = item[0], item[1]\n",
    "                if k in di:\n",
    "                    di[k] = v\n",
    "            ret.append(di)\n",
    "        ret = pd.DataFrame(ret)\n",
    "        cols = [f'{colname}_{i}_count' for i in range(len(ret.columns))]\n",
    "        ret.columns = cols\n",
    "\n",
    "        cnts = ret.sum(1)\n",
    "\n",
    "        for col in cols:\n",
    "            if col in self.idf.keys():\n",
    "                idf = self.idf[col]\n",
    "            else:\n",
    "                idf = df.shape[0] / (ret[col].sum() + 1)\n",
    "                idf = np.log(idf)\n",
    "                self.idf[col] = idf\n",
    "            \n",
    "            ret[col] = 1 + np.log(ret[col] / cnts)\n",
    "            ret[col] *= idf\n",
    "\n",
    "        return ret\n",
    "    \n",
    "    #对出现的文本单词类型进行TF-IDF构建\n",
    "    def text_change_counts(self, df):\n",
    "        tmp_df = df.groupby('id').agg({'text_change': list}).reset_index()\n",
    "        ret = list()\n",
    "        for li in tqdm(tmp_df['text_change'].values):\n",
    "            items = list(Counter(li).items())\n",
    "            di = dict()\n",
    "            for k in self.text_changes:\n",
    "                di[k] = 0\n",
    "            for item in items:\n",
    "                k, v = item[0], item[1]\n",
    "                if k in di:\n",
    "                    di[k] = v\n",
    "            ret.append(di)\n",
    "        ret = pd.DataFrame(ret)\n",
    "        cols = [f'text_change_{i}_count' for i in range(len(ret.columns))]\n",
    "        ret.columns = cols\n",
    "\n",
    "        cnts = ret.sum(1)\n",
    "\n",
    "        for col in cols:\n",
    "            if col in self.idf.keys():\n",
    "                idf = self.idf[col]\n",
    "            else:\n",
    "                idf = df.shape[0] / (ret[col].sum() + 1)\n",
    "                idf = np.log(idf)\n",
    "                self.idf[col] = idf\n",
    "            \n",
    "            ret[col] = 1 + np.log(ret[col] / cnts)\n",
    "            ret[col] *= idf\n",
    "            \n",
    "        return ret\n",
    "\n",
    "    def match_punctuations(self, df):\n",
    "        tmp_df = df.groupby('id').agg({'down_event': list}).reset_index()\n",
    "        ret = list()\n",
    "        for li in tqdm(tmp_df['down_event'].values):\n",
    "            cnt = 0\n",
    "            items = list(Counter(li).items())\n",
    "            for item in items:\n",
    "                k, v = item[0], item[1]\n",
    "                if k in self.punctuations:\n",
    "                    cnt += v\n",
    "            ret.append(cnt)\n",
    "        ret = pd.DataFrame({'punct_cnt': ret})\n",
    "        return ret\n",
    "\n",
    "\n",
    "    def get_input_words(self, df):\n",
    "        tmp_df = df[(~df['text_change'].str.contains('=>'))&(df['text_change'] != 'NoChange')].reset_index(drop=True)\n",
    "        #把每一个文章的textchange转化为list\n",
    "        tmp_df = tmp_df.groupby('id').agg({'text_change': list}).reset_index()\n",
    "        tmp_df['text_change'] = tmp_df['text_change'].apply(lambda x: ''.join(x))\n",
    "        #把每一篇文章的textchage转换为word组合\n",
    "        tmp_df['text_change'] = tmp_df['text_change'].apply(lambda x: re.findall(r'q+', x))\n",
    "        #input_word_count代表每一篇文章的单词个数\n",
    "        tmp_df['input_word_count'] = tmp_df['text_change'].apply(len)\n",
    "        #input_word_length_mean代表每一篇文章的平均字符数\n",
    "        tmp_df['input_word_length_mean'] = tmp_df['text_change'].apply(lambda x: np.mean([len(i) for i in x] if len(x) > 0 else 0))\n",
    "        #input_word_length_pct_代表每一篇文章的单词个数的百分数\n",
    "        for percentile in self.percentiles:\n",
    "            tmp_df[f'input_word_length_pct_{percentile}'] = tmp_df['text_change'].apply(lambda x: np.percentile([len(i) for i in x] if len(x) > 0 else 0, \n",
    "                                                                                                                percentile))\n",
    "        #input_word_length_max代表每个文章的最大单词                                                                                                      \n",
    "        tmp_df['input_word_length_max'] = tmp_df['text_change'].apply(lambda x: np.max([len(i) for i in x] if len(x) > 0 else 0))\n",
    "        #input_word_length_std代表每个文章的单词长度的标准差\n",
    "        tmp_df['input_word_length_std'] = tmp_df['text_change'].apply(lambda x: np.std([len(i) for i in x] if len(x) > 0 else 0))\n",
    "        tmp_df.drop(['text_change'], axis=1, inplace=True)\n",
    "        return tmp_df\n",
    "\n",
    "    def make_feats(self, df):\n",
    "        \n",
    "        feats = pd.DataFrame({'id': df['id'].unique().tolist()})\n",
    "        \n",
    "        #时间做差\n",
    "        print(\"Engineering time data\")\n",
    "        for gap in self.gaps:\n",
    "            df[f'up_time_shift{gap}'] = df.groupby('id')['up_time'].shift(gap)\n",
    "            df[f'action_time_gap{gap}'] = df['down_time'] - df[f'up_time_shift{gap}']\n",
    "        df.drop(columns=[f'up_time_shift{gap}' for gap in self.gaps], inplace=True)\n",
    "\n",
    "        #对文章的光标进行做差\n",
    "        print(\"Engineering cursor position data\")\n",
    "        for gap in self.gaps:\n",
    "            df[f'cursor_position_shift{gap}'] = df.groupby('id')['cursor_position'].shift(gap)\n",
    "            df[f'cursor_position_change{gap}'] = df['cursor_position'] - df[f'cursor_position_shift{gap}']\n",
    "            df[f'cursor_position_abs_change{gap}'] = np.abs(df[f'cursor_position_change{gap}'])\n",
    "        df.drop(columns=[f'cursor_position_shift{gap}' for gap in self.gaps], inplace=True)\n",
    "\n",
    "        #对每一篇文章的字数做差\n",
    "        print(\"Engineering word count data\")\n",
    "        for gap in self.gaps:\n",
    "            df[f'word_count_shift{gap}'] = df.groupby('id')['word_count'].shift(gap)\n",
    "            df[f'word_count_change{gap}'] = df['word_count'] - df[f'word_count_shift{gap}']\n",
    "            df[f'word_count_abs_change{gap}'] = np.abs(df[f'word_count_change{gap}'])\n",
    "        df.drop(columns=[f'word_count_shift{gap}' for gap in self.gaps], inplace=True)\n",
    "        \n",
    "        '''\n",
    "        'event_id', ['max']获取每一篇文章的操作数\n",
    "        ('up_time', ['max'])获取每一篇文章截至时间\n",
    "        'action_time'获取每一篇文章操作时间的统计信息\n",
    "        '''\n",
    "        print(\"Engineering statistical summaries for features\")\n",
    "        feats_stat = [\n",
    "            ('event_id', ['max']),\n",
    "            ('up_time', ['max']),\n",
    "            ('action_time', ['max', 'min', 'mean', 'std', 'quantile', 'sem', 'sum', 'skew', pd.DataFrame.kurt]),\n",
    "            ('activity', ['nunique']),\n",
    "            ('down_event', ['nunique']),\n",
    "            ('up_event', ['nunique']),\n",
    "            ('text_change', ['nunique']),\n",
    "            ('cursor_position', ['nunique', 'max', 'quantile', 'sem', 'mean']),\n",
    "            ('word_count', ['nunique', 'max', 'quantile', 'sem', 'mean'])]\n",
    "        for gap in self.gaps:\n",
    "            feats_stat.extend([\n",
    "                (f'action_time_gap{gap}', ['max', 'min', 'mean', 'std', 'quantile', 'sem', 'sum', 'skew', pd.DataFrame.kurt]),\n",
    "                (f'cursor_position_change{gap}', ['max', 'mean', 'std', 'quantile', 'sem', 'sum', 'skew', pd.DataFrame.kurt]),\n",
    "                (f'word_count_change{gap}', ['max', 'mean', 'std', 'quantile', 'sem', 'sum', 'skew', pd.DataFrame.kurt])\n",
    "            ])\n",
    "        \n",
    "        pbar = tqdm(feats_stat)\n",
    "        for item in pbar:\n",
    "            colname, methods = item[0], item[1]\n",
    "            for method in methods:\n",
    "                pbar.set_postfix()\n",
    "                if isinstance(method, str):\n",
    "                    method_name = method\n",
    "                else:\n",
    "                    method_name = method.__name__\n",
    "                pbar.set_postfix(column=colname, method=method_name)\n",
    "                tmp_df = df.groupby(['id']).agg({colname: method}).reset_index().rename(columns={colname: f'{colname}_{method_name}'})\n",
    "                feats = feats.merge(tmp_df, on='id', how='left')\n",
    "\n",
    "        print(\"Engineering activity counts data\")\n",
    "        tmp_df = self.activity_counts(df)\n",
    "        feats = pd.concat([feats, tmp_df], axis=1)\n",
    "        \n",
    "        print(\"Engineering event counts data\")\n",
    "        tmp_df = self.event_counts(df, 'down_event')\n",
    "        feats = pd.concat([feats, tmp_df], axis=1)\n",
    "        tmp_df = self.event_counts(df, 'up_event')\n",
    "        feats = pd.concat([feats, tmp_df], axis=1)\n",
    "        \n",
    "        print(\"Engineering text change counts data\")\n",
    "        tmp_df = self.text_change_counts(df)\n",
    "        feats = pd.concat([feats, tmp_df], axis=1)\n",
    "        \n",
    "        print(\"Engineering punctuation counts data\")\n",
    "        tmp_df = self.match_punctuations(df)\n",
    "        feats = pd.concat([feats, tmp_df], axis=1)\n",
    "\n",
    "        print(\"Engineering input words data\")\n",
    "        tmp_df = self.get_input_words(df)\n",
    "        feats = pd.merge(feats, tmp_df, on='id', how='left')\n",
    "\n",
    "        print(\"Engineering ratios data\")\n",
    "        #每一篇文章process每分钟写的字数\n",
    "        feats['word_time_ratio'] = feats['word_count_max'] / feats['up_time_max']\n",
    "        feats['word_event_ratio'] = feats['word_count_max'] / feats['event_id_max']\n",
    "        feats['event_time_ratio'] = feats['event_id_max']  / feats['up_time_max']\n",
    "        feats['idle_time_ratio'] = feats['action_time_gap1_sum'] / feats['up_time_max']\n",
    "\n",
    "        return feats   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering time data\n",
      "Engineering cursor position data\n",
      "Engineering word count data\n",
      "Engineering statistical summaries for features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [03:15<00:00,  5.92s/it, column=word_count_change100, method=kurt]         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering activity counts data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2471/2471 [00:00<00:00, 7395.86it/s]\n",
      "c:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\core\\arraylike.py:358: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering event counts data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2471/2471 [00:00<00:00, 5108.48it/s]\n",
      "100%|██████████| 2471/2471 [00:00<00:00, 7232.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering text change counts data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2471/2471 [00:00<00:00, 6194.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering punctuation counts data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2471/2471 [00:00<00:00, 7202.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering input words data\n",
      "Engineering ratios data\n",
      "Engineering time data\n",
      "Engineering cursor position data\n",
      "Engineering word count data\n",
      "Engineering statistical summaries for features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [00:03<00:00,  8.83it/s, column=word_count_change100, method=kurt]         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering activity counts data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering event counts data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 3010.99it/s]\n",
      "100%|██████████| 3/3 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering text change counts data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering punctuation counts data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering input words data\n",
      "Engineering ratios data\n"
     ]
    }
   ],
   "source": [
    "#初始化precessor初始化类\n",
    "preprocessor = Preprocessor(seed=42)\n",
    "#对train和test数据集进行特征产生\n",
    "train_feats = preprocessor.make_feats(train_logs)\n",
    "test_feats = preprocessor.make_feats(test_logs)\n",
    "nan_cols = train_feats.columns[train_feats.isna().any()].tolist()\n",
    "train_feats = train_feats.drop(columns=nan_cols)\n",
    "test_feats = test_feats.drop(columns=nan_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_agg_fe_df = train_logs.groupby(\"id\")[['down_time', 'up_time', 'action_time', 'cursor_position', 'word_count']].agg(\n",
    "    ['mean', 'std', 'min', 'max', 'last', 'first', 'sem', 'median', 'sum'])\n",
    "train_agg_fe_df.columns = ['_'.join(x) for x in train_agg_fe_df.columns]\n",
    "train_agg_fe_df = train_agg_fe_df.add_prefix(\"tmp_\")\n",
    "train_agg_fe_df.reset_index(inplace=True)\n",
    "\n",
    "test_agg_fe_df = test_logs.groupby(\"id\")[['down_time', 'up_time', 'action_time', 'cursor_position', 'word_count']].agg(\n",
    "    ['mean', 'std', 'min', 'max', 'last', 'first', 'sem', 'median', 'sum'])\n",
    "test_agg_fe_df.columns = ['_'.join(x) for x in test_agg_fe_df.columns]\n",
    "test_agg_fe_df = test_agg_fe_df.add_prefix(\"tmp_\")\n",
    "test_agg_fe_df.reset_index(inplace=True)\n",
    "\n",
    "train_feats = train_feats.merge(train_agg_fe_df, on='id', how='left')\n",
    "test_feats = test_feats.merge(test_agg_fe_df, on='id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>event_id_max</th>\n",
       "      <th>up_time_max</th>\n",
       "      <th>action_time_max</th>\n",
       "      <th>action_time_min</th>\n",
       "      <th>action_time_mean</th>\n",
       "      <th>action_time_std</th>\n",
       "      <th>action_time_quantile</th>\n",
       "      <th>action_time_sem</th>\n",
       "      <th>action_time_sum</th>\n",
       "      <th>...</th>\n",
       "      <th>tmp_cursor_position_sum</th>\n",
       "      <th>tmp_word_count_mean</th>\n",
       "      <th>tmp_word_count_std</th>\n",
       "      <th>tmp_word_count_min</th>\n",
       "      <th>tmp_word_count_max</th>\n",
       "      <th>tmp_word_count_last</th>\n",
       "      <th>tmp_word_count_first</th>\n",
       "      <th>tmp_word_count_sem</th>\n",
       "      <th>tmp_word_count_median</th>\n",
       "      <th>tmp_word_count_sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>001519c8</td>\n",
       "      <td>2557</td>\n",
       "      <td>1801969</td>\n",
       "      <td>2259</td>\n",
       "      <td>0</td>\n",
       "      <td>116.246774</td>\n",
       "      <td>91.797374</td>\n",
       "      <td>112.0</td>\n",
       "      <td>1.815369</td>\n",
       "      <td>297243</td>\n",
       "      <td>...</td>\n",
       "      <td>1818445</td>\n",
       "      <td>128.116152</td>\n",
       "      <td>76.498372</td>\n",
       "      <td>0</td>\n",
       "      <td>256</td>\n",
       "      <td>255</td>\n",
       "      <td>0</td>\n",
       "      <td>1.512819</td>\n",
       "      <td>132.0</td>\n",
       "      <td>327593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0022f953</td>\n",
       "      <td>2454</td>\n",
       "      <td>1788969</td>\n",
       "      <td>1758</td>\n",
       "      <td>0</td>\n",
       "      <td>112.221271</td>\n",
       "      <td>55.431189</td>\n",
       "      <td>115.0</td>\n",
       "      <td>1.118966</td>\n",
       "      <td>275391</td>\n",
       "      <td>...</td>\n",
       "      <td>1904809</td>\n",
       "      <td>182.714751</td>\n",
       "      <td>97.763090</td>\n",
       "      <td>0</td>\n",
       "      <td>323</td>\n",
       "      <td>320</td>\n",
       "      <td>0</td>\n",
       "      <td>1.973502</td>\n",
       "      <td>186.0</td>\n",
       "      <td>448382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0042269b</td>\n",
       "      <td>4136</td>\n",
       "      <td>1771669</td>\n",
       "      <td>3005</td>\n",
       "      <td>0</td>\n",
       "      <td>101.837766</td>\n",
       "      <td>82.383766</td>\n",
       "      <td>94.0</td>\n",
       "      <td>1.281007</td>\n",
       "      <td>421201</td>\n",
       "      <td>...</td>\n",
       "      <td>3025946</td>\n",
       "      <td>194.772727</td>\n",
       "      <td>108.935068</td>\n",
       "      <td>0</td>\n",
       "      <td>404</td>\n",
       "      <td>404</td>\n",
       "      <td>0</td>\n",
       "      <td>1.693860</td>\n",
       "      <td>193.0</td>\n",
       "      <td>805580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0059420b</td>\n",
       "      <td>1556</td>\n",
       "      <td>1404469</td>\n",
       "      <td>806</td>\n",
       "      <td>0</td>\n",
       "      <td>121.848329</td>\n",
       "      <td>113.768226</td>\n",
       "      <td>110.0</td>\n",
       "      <td>2.884139</td>\n",
       "      <td>189596</td>\n",
       "      <td>...</td>\n",
       "      <td>844188</td>\n",
       "      <td>103.618895</td>\n",
       "      <td>61.882250</td>\n",
       "      <td>0</td>\n",
       "      <td>206</td>\n",
       "      <td>206</td>\n",
       "      <td>0</td>\n",
       "      <td>1.568777</td>\n",
       "      <td>108.5</td>\n",
       "      <td>161231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0075873a</td>\n",
       "      <td>2531</td>\n",
       "      <td>1662472</td>\n",
       "      <td>701</td>\n",
       "      <td>0</td>\n",
       "      <td>123.943896</td>\n",
       "      <td>62.082013</td>\n",
       "      <td>129.0</td>\n",
       "      <td>1.234013</td>\n",
       "      <td>313702</td>\n",
       "      <td>...</td>\n",
       "      <td>1518729</td>\n",
       "      <td>125.082971</td>\n",
       "      <td>77.255054</td>\n",
       "      <td>0</td>\n",
       "      <td>252</td>\n",
       "      <td>252</td>\n",
       "      <td>0</td>\n",
       "      <td>1.535610</td>\n",
       "      <td>113.0</td>\n",
       "      <td>316585</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 339 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  event_id_max  up_time_max  action_time_max  action_time_min  \\\n",
       "0  001519c8          2557      1801969             2259                0   \n",
       "1  0022f953          2454      1788969             1758                0   \n",
       "2  0042269b          4136      1771669             3005                0   \n",
       "3  0059420b          1556      1404469              806                0   \n",
       "4  0075873a          2531      1662472              701                0   \n",
       "\n",
       "   action_time_mean  action_time_std  action_time_quantile  action_time_sem  \\\n",
       "0        116.246774        91.797374                 112.0         1.815369   \n",
       "1        112.221271        55.431189                 115.0         1.118966   \n",
       "2        101.837766        82.383766                  94.0         1.281007   \n",
       "3        121.848329       113.768226                 110.0         2.884139   \n",
       "4        123.943896        62.082013                 129.0         1.234013   \n",
       "\n",
       "   action_time_sum  ...  tmp_cursor_position_sum  tmp_word_count_mean  \\\n",
       "0           297243  ...                  1818445           128.116152   \n",
       "1           275391  ...                  1904809           182.714751   \n",
       "2           421201  ...                  3025946           194.772727   \n",
       "3           189596  ...                   844188           103.618895   \n",
       "4           313702  ...                  1518729           125.082971   \n",
       "\n",
       "   tmp_word_count_std  tmp_word_count_min  tmp_word_count_max  \\\n",
       "0           76.498372                   0                 256   \n",
       "1           97.763090                   0                 323   \n",
       "2          108.935068                   0                 404   \n",
       "3           61.882250                   0                 206   \n",
       "4           77.255054                   0                 252   \n",
       "\n",
       "   tmp_word_count_last  tmp_word_count_first  tmp_word_count_sem  \\\n",
       "0                  255                     0            1.512819   \n",
       "1                  320                     0            1.973502   \n",
       "2                  404                     0            1.693860   \n",
       "3                  206                     0            1.568777   \n",
       "4                  252                     0            1.535610   \n",
       "\n",
       "   tmp_word_count_median  tmp_word_count_sum  \n",
       "0                  132.0              327593  \n",
       "1                  186.0              448382  \n",
       "2                  193.0              805580  \n",
       "3                  108.5              161231  \n",
       "4                  113.0              316585  \n",
       "\n",
       "[5 rows x 339 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_feats.head(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
